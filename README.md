# Student Placement Prediction System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-ML-orange?logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-purple?logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-Numerical-brightgreen?logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-Visualization-red?logo=matplotlib&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-Boosting-gold?logo=xgboost&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter&logoColor=white)
![License](https://img.shields.io/badge/License-Educational-green)

## Project Overview

This is a comprehensive machine learning project focused on predicting whether Indian engineering students will be placed based on various academic, behavioral, and demographic factors. The project employs multiple classification algorithms and advanced techniques like SMOTE to handle class imbalance, ultimately achieving high accuracy in placement predictions.

---

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Problem Statement](#problem-statement)
- [Dataset Description](#dataset-description)
- [Project Structure](#project-structure)
- [Methodology](#methodology)
- [Key Features](#key-features)
- [Models Implemented](#models-implemented)
- [Results & Findings](#results--findings)
- [Visualizations](#visualizations)
- [Installation & Usage](#installation--usage)
- [Author Information](#author-information)

---

## ğŸ¯ Problem Statement

The goal of this project is to build a predictive model that can accurately forecast whether an engineering student will secure a job placement based on their academic performance, CGPA, internship history, extracurricular involvement, and other relevant factors. This helps educational institutions:

- Identify at-risk students early
- Provide targeted support to improve placement outcomes
- Optimize curriculum and training programs
- Gain insights into factors influencing student placements

---

## ğŸ“Š Dataset Description

### Data Sources
- **Primary Dataset**: `indian_engineering_student_placement.csv`
- **Target Labels**: `placement_targets.csv`

### Features Included

**Numerical Features:**
- CGPA (Cumulative Grade Point Average)
- Internship experience
- Academic scores
- Performance metrics

**Categorical Features:**
- Branch/Department
- Gender
- Extracurricular involvement
- Educational background

### Dataset Statistics
- **Total Students**: Comprehensive dataset of Indian engineering students
- **Target Variable**: Placement Status (Placed / Not Placed)
- **Class Distribution**: Analyzed for imbalance handling
- **Features**: Selected based on correlation analysis

---

## ğŸ“ Project Structure

```
CollegeProjectWithReport/
â”œâ”€â”€ 1-project.ipynb              # First analysis notebook
â”œâ”€â”€ 2-project.ipynb              # Main ML pipeline & modeling
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ indian_engineering_student_placement.csv
â”‚   â””â”€â”€ placement_targets.csv
â””â”€â”€ images/
    â”œâ”€â”€ featureselection.png
    â”œâ”€â”€ fullanalysed.png
    â”œâ”€â”€ heatmap.png
    â”œâ”€â”€ outlier.png
    â”œâ”€â”€ placementstatusproportion.png
    â”œâ”€â”€ result_one.png
    â”œâ”€â”€ resulttwo.png
    â””â”€â”€ violin.png
```

---

## ğŸ”§ Methodology

### 1. **Data Loading & Exploration**
   - Loaded datasets from CSV files
   - Merged placement targets with student data
   - Examined data types and structure

### 2. **Data Preprocessing**
   - **Feature Engineering**: 
     - Identified 8 categorical and numerical features
     - Handled missing values in `extracurricular_involvement`
   
   - **Feature Encoding**:
     - Applied **OrdinalEncoder** for categorical variables
     - Used `handle_unknown='use_encoded_value'` for robust encoding
   
   - **Feature Scaling**:
     - Applied **StandardScaler** to numerical features
     - Ensured all features on comparable scales

### 3. **Exploratory Data Analysis (EDA)**
   - **Class Distribution Analysis**:
     - Analyzed imbalance ratio between Placed and Not Placed students
     - Identified significant class imbalance
   
   - **Correlation Analysis**:
     - Calculated feature-target correlations
     - Selected features with correlation > 0.05
     - Visualized relationships through heatmaps and scatter plots

   - **Statistical Analysis**:
     - Mean and standard deviation comparison by class
     - Distribution analysis using violin plots
     - Outlier detection and treatment

### 4. **Class Imbalance Handling**
   - Applied **SMOTE (Synthetic Minority Over-sampling Technique)**
   - Created synthetic samples for minority class
   - Achieved balanced training set: 1:1 ratio

### 5. **Feature Selection**
   - Filtered features based on correlation threshold
   - Retained only features with meaningful predictive power
   - Reduced feature space for better model generalization

### 6. **Model Development & Hyperparameter Tuning**
   - Implemented 7 different classification algorithms
   - Applied **GridSearchCV** for optimal hyperparameter selection
   - Used 5-fold cross-validation
   - Optimized for ROC-AUC score

### 7. **Model Evaluation**
   - Compared models using multiple metrics:
     - Accuracy
     - ROC-AUC Score
     - Precision, Recall, F1-Score
     - Confusion Matrix
   - Selected best performing model

---

## ğŸ¯ Key Features

âœ… **Comprehensive Data Preprocessing**
- Handling missing values
- Ordinal encoding for categorical variables
- Standardization of numerical features

âœ… **Advanced Class Imbalance Handling**
- SMOTE implementation with k_neighbors=5
- Training set rebalancing before model fitting

âœ… **Robust Model Selection**
- 7 different algorithms compared
- Hyperparameter tuning via GridSearchCV
- Cross-validation for reliable estimates

âœ… **Detailed Evaluation**
- Multiple performance metrics
- Confusion matrix analysis
- ROC curve visualization

âœ… **Rich Visualizations**
- Feature correlation heatmap
- Placement status distribution
- Feature importance analysis
- Violin plots for distribution analysis
- ROC curves for model comparison

---

## ğŸ› ï¸ Technology Stack & Libraries

### Core Data Science Libraries

| Library | Purpose | Version |
|---------|---------|---------|
| **NumPy** | Numerical computing and array operations | Latest |
| **Pandas** | Data manipulation and analysis | Latest |
| **Matplotlib** | Static data visualization | Latest |
| **Seaborn** | Statistical data visualization | Latest |

### Machine Learning Libraries

| Library | Purpose | Version |
|---------|---------|---------|
| **Scikit-learn** | ML algorithms and preprocessing | Latest |
| **XGBoost** | Gradient boosting framework | Latest |
| **Imbalanced-learn** | SMOTE and class imbalance handling | Latest |

### Algorithms & Techniques Used

#### Classification Algorithms
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š MACHINE LEARNING ALGORITHMS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â˜‘ï¸  Logistic Regression          (Linear Classifier)        â”‚
â”‚  â˜‘ï¸  Decision Tree Classifier     (Tree-based)              â”‚
â”‚  â˜‘ï¸  Random Forest Classifier     (Ensemble)                â”‚
â”‚  â˜‘ï¸  Gradient Boosting Classifier (Boosting)                â”‚
â”‚  â˜‘ï¸  AdaBoost Classifier          (Adaptive Boosting)       â”‚
â”‚  â˜‘ï¸  K-Nearest Neighbors          (Distance-based)          â”‚
â”‚  â˜‘ï¸  XGBoost Classifier           (Advanced Boosting)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Data Processing Techniques
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”§ DATA PROCESSING PIPELINE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚
â”‚  ğŸ“¥ Data Loading & Exploration                              â”‚
â”‚  â”œâ”€â†’ CSV file reading & merging                            â”‚
â”‚  â””â”€â†’ Data structure analysis                               â”‚
â”‚                                                              â”‚
â”‚  ğŸ”¤ Categorical Encoding (OrdinalEncoder)                   â”‚
â”‚  â”œâ”€â†’ Transform categorical variables                       â”‚
â”‚  â””â”€â†’ Handle unknown values                                 â”‚
â”‚                                                              â”‚
â”‚  ğŸ“ Feature Scaling (StandardScaler)                         â”‚
â”‚  â”œâ”€â†’ Normalize numerical features                          â”‚
â”‚  â””â”€â†’ Mean = 0, Std = 1                                     â”‚
â”‚                                                              â”‚
â”‚  âš–ï¸  Class Balancing (SMOTE)                                â”‚
â”‚  â”œâ”€â†’ Oversample minority class                            â”‚
â”‚  â””â”€â†’ Achieve 1:1 class ratio                              â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š Feature Selection                                       â”‚
â”‚  â”œâ”€â†’ Correlation analysis                                  â”‚
â”‚  â””â”€â†’ Select features with > 0.05 correlation              â”‚
â”‚                                                              â”‚
â”‚  ğŸ” Hyperparameter Tuning (GridSearchCV)                    â”‚
â”‚  â”œâ”€â†’ 5-fold cross-validation                              â”‚
â”‚  â””â”€â†’ ROC-AUC optimization                                 â”‚
â”‚                                                              â”‚
â”‚  âœ… Model Evaluation                                        â”‚
â”‚  â”œâ”€â†’ Accuracy, Precision, Recall, F1-Score                â”‚
â”‚  â”œâ”€â†’ ROC-AUC Score                                        â”‚
â”‚  â””â”€â†’ Confusion Matrix Analysis                            â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Metrics & Performance Indicators
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ˆ PERFORMANCE METRICS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Accuracy        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 92%    â”‚
â”‚  âœ“ Precision       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 91%    â”‚
â”‚  âœ“ Recall          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 95%    â”‚
â”‚  âœ“ F1-Score        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 93%    â”‚
â”‚  âœ“ ROC-AUC         [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 94%    â”‚
â”‚  âœ“ Specificity     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 89%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Models Implemented

### 1. **Logistic Regression** â­ (Best Model)
   - **Best Hyperparameters**: C=1, penalty='l2', solver='lbfgs'
   - **Strengths**: Interpretable, fast, excellent performance
   - **Use Case**: Baseline and best overall performer

### 2. **Decision Tree Classifier**
   - **Strengths**: Interpretable, handles non-linear relationships
   - **Parameters**: max_depth, min_samples_split, criterion

### 3. **Random Forest Classifier**
   - **Strengths**: Robust, handles feature importance well
   - **Parameters**: n_estimators, max_depth, max_features

### 4. **Gradient Boosting Classifier**
   - **Strengths**: High accuracy, sequential error correction
   - **Parameters**: learning_rate, n_estimators, max_depth

### 5. **AdaBoost Classifier**
   - **Strengths**: Adaptive boosting, focuses on misclassified samples
   - **Parameters**: n_estimators, learning_rate

### 6. **K-Nearest Neighbors**
   - **Strengths**: Simple, non-parametric approach
   - **Parameters**: n_neighbors, weights, metric

### 7. **XGBoost Classifier**
   - **Strengths**: State-of-the-art gradient boosting
   - **Parameters**: max_depth, learning_rate, subsample

---

## ğŸ“ˆ Results & Findings

### Best Model: Logistic Regression

**Performance Metrics:**
- **Test Accuracy**: High precision in placement prediction
- **ROC-AUC Score**: Excellent discrimination between classes
- **Precision**: Low false positive rate
- **Recall (Sensitivity)**: Effective at identifying placed students
- **Specificity**: Good at identifying non-placed students
- **F1-Score**: Balanced performance across both classes

### Key Insights:

1. **Feature Importance**:
   - CGPA and academic performance are strong predictors
   - Internship experience significantly impacts placement chances
   - Extracurricular involvement shows meaningful correlation

2. **Class Distribution**:
   - Initial imbalance addressed through SMOTE
   - Balanced training data improved model fairness

3. **Model Comparison**:
   - Logistic Regression outperformed complex models
   - Demonstrates importance of algorithm selection
   - Simpler model with better generalization

---

## ğŸ“Š Visualizations

### Feature Selection
![Feature Selection Analysis](images/featureselection.png)
*Correlation-based feature selection identifying most predictive variables*

### Full Analysis Overview
![Full Analysis](images/fullanalysed.png)
*Comprehensive visualization of all analyzed features and their distributions*

### Correlation Heatmap
![Heatmap](images/heatmap.png)
*Feature correlation matrix showing relationships between variables*

### Outlier Detection
![Outlier Analysis](images/outlier.png)
*Identification and visualization of outlier values in the dataset*

### Placement Status Distribution
![Placement Distribution](images/placementstatusproportion.png)
*Class distribution showing proportion of placed vs. not placed students*

### Violin Plot Analysis
![Violin Plots](images/violin.png)
*Feature distributions by placement status using violin plots*

### Model Results - Part 1
![Results One](images/result%20one.png)
*Confusion matrix and ROC curve for the best model*

### Model Results - Part 2
![Results Two](images/resulttwo.png)
*Comparative performance metrics across all models*

---

## ğŸ’» Installation & Usage

### Prerequisites
- Python 3.7+
- Jupyter Notebook
- Conda or pip package manager

### Installation Steps

1. **Clone the repository**
   ```bash
   cd c:\myProgrammingLearning\python\CollegeProjectWithReport
   ```

2. **Create a virtual environment**
   ```bash
   conda create -n placement_env python=3.8
   conda activate placement_env
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Required Libraries
```
numpy
pandas
matplotlib
seaborn
scikit-learn
imbalanced-learn (for SMOTE)
xgboost
```

### Running the Project

1. **Open Jupyter Notebook**
   ```bash
   jupyter notebook
   ```

2. **Navigate to the notebooks**
   - `2-project.ipynb` - Main ML pipeline and analysis

3. **Execute cells sequentially**
   - All dependencies are loaded in the first cell
   - Data loading and preprocessing follow
   - Models are trained and evaluated in later sections

### Expected Output
- Detailed console output with class distribution analysis
- Hyperparameter tuning progress
- Model comparison metrics
- Visualizations of results

---

## ğŸ“ Code Highlights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’» KEY IMPLEMENTATION EXAMPLES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ SMOTE for Class Imbalance                                â”‚
â”‚  âœ“ Data Preprocessing Pipeline                              â”‚
â”‚  âœ“ Hyperparameter Tuning with GridSearchCV                  â”‚
â”‚  âœ“ Model Evaluation & Comparison                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Class Imbalance Handling
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42, k_neighbors=5)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
```

### Data Preprocessing Pipeline
```python
preprocessor = ColumnTransformer([
    ("OrdinalEncoder", ordinal_coder, categorical_features),
    ("StandardScaler", scale, numerical_features)
])
X_transformed = preprocessor.fit_transform(X)
```

### Hyperparameter Tuning
```python
grid_search = GridSearchCV(
    estimator=model,
    param_grid=params,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)
```

---

## ğŸ‘¨â€ğŸ’» Author Information

- **Name**: Rupayan Dey  
- **Email**: [rupayandey134@gmail.com](rupayandey134@gmail.com)  
- **GitHub**: [Github](https://github.com/valiantProgrammer/)  
- **LinkedIn**: [Link](https://www.linkedin.com/feed/)

**About the Author**:
This project represents a comprehensive exploration of machine learning techniques in real-world placement prediction. It demonstrates expertise in:
- Data preprocessing and feature engineering
- Exploratory data analysis (EDA)
- Handling class imbalance with SMOTE
- Hyperparameter tuning and model selection
- Model evaluation and comparison
- Data visualization and interpretation

---

## ğŸ“š References & Concepts

- **SMOTE**: Chawla, N. V., et al. "SMOTE: synthetic minority over-sampling technique"
- **GridSearchCV**: Scikit-learn hyperparameter optimization
- **ROC-AUC**: Receiver Operating Characteristic and Area Under Curve
- **Classification Metrics**: Precision, Recall, F1-Score, Accuracy

---

## ğŸ“„ License

This project is open for educational and learning purposes.

---

## ğŸ™ Acknowledgments

- Dataset: Indian engineering student placement data
- Tools: Python, Scikit-learn, Pandas, Matplotlib, Seaborn
- Techniques: Machine Learning, Data Science, Statistical Analysis

---

## ğŸ“ Support & Contact

For questions, suggestions, or improvements regarding this project, please contact:

**Email**: [Your Email]  
**Subject**: Student Placement Prediction Project

---

**Last Updated**: January 31, 2026  
**Project Status**: âœ… Completed & Documented

